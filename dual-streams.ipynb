{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall tensorflow-io -y\n",
    "!pip install --no-deps tensorflow-io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-12T17:11:22.051723Z",
     "iopub.status.busy": "2023-06-12T17:11:22.050750Z",
     "iopub.status.idle": "2023-06-12T17:11:22.059370Z",
     "shell.execute_reply": "2023-06-12T17:11:22.058140Z",
     "shell.execute_reply.started": "2023-06-12T17:11:22.051688Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from numpy.random import seed\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import join\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train=\"../input/rca-lca/train\"\n",
    "path_test=\"../input/rca-lca/Test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_all=\"../input/rca-lca\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "nb_epochs=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255,       \n",
    "        rotation_range = 20 , # Rotate images randomly within the range of -20 to +20 degrees\n",
    "        width_shift_range = 0.1 , # Randomly shift images horizontally within the range of -10% to +10%\n",
    "        height_shift_range = 0.1 , # Randomly shift images vertically within the range of -10% to +10%\n",
    "        zoom_range = 0.2 , # Randomly zoom images within the range of 80% to 120%\n",
    "        horizontal_flip = True , # Randomly flip images horizontally\n",
    "        vertical_flip = True , # Randomly flip images vertically\n",
    "        validation_split=0.2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = datagen.flow_from_directory(path_all,\n",
    "                                                subset='training',\n",
    "                                                target_size=(128, 128),\n",
    "                                                batch_size=28, #batch_size,\n",
    "                                                validation=0.2,\n",
    "                                                class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_generator = datagen.flow_from_directory(\n",
    "    path_all, # same directory as training data\n",
    "    subset='validation',\n",
    "    target_size=(128, 128),\n",
    "    batch_size=28,\n",
    "    class_mode='binary' # Type of labels (e.g., binary or categorical)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator.image_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator.target_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(path_test,\n",
    "                                                  target_size=(128, 128),\n",
    "                                                  batch_size=28,\n",
    "                                                  class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dual_stream_model=\"best_DS_Model.h5\"\n",
    "checkpoint = ModelCheckpoint(dual_stream_model,  # model filename\n",
    "                             monitor='loss',\n",
    "                             verbose=1, # verbosity - 0 or 1\n",
    "                             save_best_only= True, \n",
    "                             mode='auto') \n",
    "early_stopping = EarlyStopping(monitor='loss',\n",
    "                               patience=5,\n",
    "                               verbose=1,\n",
    "                               mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-12T17:11:34.275705Z",
     "iopub.status.busy": "2023-06-12T17:11:34.275352Z",
     "iopub.status.idle": "2023-06-12T17:11:34.282090Z",
     "shell.execute_reply": "2023-06-12T17:11:34.281103Z",
     "shell.execute_reply.started": "2023-06-12T17:11:34.275676Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import plot_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-12T17:10:44.316113Z",
     "iopub.status.busy": "2023-06-12T17:10:44.315818Z",
     "iopub.status.idle": "2023-06-12T17:10:44.321176Z",
     "shell.execute_reply": "2023-06-12T17:10:44.320285Z",
     "shell.execute_reply.started": "2023-06-12T17:10:44.316089Z"
    }
   },
   "outputs": [],
   "source": [
    "def drawModel(model_2streams):\n",
    "    # Tracer le modèle\n",
    "    plot_model(model_2streams, to_file='model_2streams.png', \n",
    "               show_shapes=True, \n",
    "               show_layer_names=True)\n",
    "    \n",
    "    image = plt.imread('model_2streams.png')\n",
    "    \n",
    "    plt.figure(figsize=(20, 40))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========== Dual-Dense ==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (128, 128, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = tf.keras.layers.Input(shape=input_shape, name=\"left\")\n",
    "hidden1A = tf.keras.layers.Dense(16, activation=\"relu\")(input_A)\n",
    "hidden2A = tf.keras.layers.Dense(32, activation=\"relu\")(hidden1A)\n",
    "hidden3A = tf.keras.layers.Dense(64, activation=\"relu\")(hidden2A)\n",
    "hidden4A = tf.keras.layers.Dense(32, activation=\"relu\")(hidden3A)\n",
    "\n",
    "\n",
    "input_B = tf.keras.layers.Input(shape=input_shape, name=\"right\")\n",
    "hidden1B = tf.keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2B = tf.keras.layers.Dense(30, activation=\"relu\")(hidden1B)\n",
    "\n",
    "concat = tf.keras.layers.concatenate([hidden4A, hidden2B])\n",
    "flatten = tf.keras.layers.Flatten()(concat)\n",
    "\n",
    "output = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"output\")(flatten)\n",
    "model = tf.keras.Model(inputs=[input_A, input_B], outputs=[output])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=train_generator.next()[1]\n",
    "labels = tf.expand_dims(labels, axis=-1)\n",
    "labels = tf.expand_dims(labels, axis=-1)\n",
    "labels = tf.expand_dims(labels, axis=-1)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = len(train_generator)\n",
    "x_train, y_train = next(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist=model.fit([x_train, x_train ], y_train,\n",
    "            steps_per_epoch=1330/batch_size,\n",
    "            epochs = 50\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = next(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_labels=train_generator.next()[1]\n",
    "#y_test = tf.expand_dims(y_test, axis=-1)\n",
    "#y_test = tf.expand_dims(y_test, axis=-1)\n",
    "#y_test = tf.expand_dims(y_test, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict([x_test, x_test])\n",
    "y_pred_classes = (Y_pred > 0.5).astype(int)  # Convert probabilities to classes (0 or 1)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------- dual cnn classique ------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, concatenate,add\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=28\n",
    "nb_epochs=50\n",
    "input_shape = (128, 128, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myCNN1():\n",
    "        model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=input_shape),\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "    ])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myCNN2():\n",
    "        model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=input_shape),\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten()\n",
    "    ])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream1 = myCNN1()\n",
    "stream1_output = stream1.output\n",
    "#stream1_output = Flatten()(stream1_output)\n",
    "\n",
    "\n",
    "stream2 = myCNN2()\n",
    "stream2_output = stream2.output\n",
    "#stream2_output = Flatten()(stream2_output)\n",
    "\n",
    "# Concatenate the outputs of both streams\n",
    "merged = concatenate([stream1_output, stream2_output])\n",
    "\n",
    "output = Dense(16, activation='relu')(merged)\n",
    "# Add a dense layer for classification\n",
    "output = Dense(1, activation='sigmoid')(output)\n",
    "\n",
    "# Create the dual-stream model\n",
    "model_2streams = Model(inputs=[stream1.input,stream2.input], outputs=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_model=\"best_2streams_CNN_Model.h5\"\n",
    "checkpoint = ModelCheckpoint(model_2streams,  # model filename\n",
    "                             monitor='loss',\n",
    "                             verbose=1, # verbosity - 0 or 1\n",
    "                             save_best_only= True, \n",
    "                             mode='auto') \n",
    "early_stopping = EarlyStopping(monitor='loss',\n",
    "                               patience=5,\n",
    "                               verbose=1,\n",
    "                               mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2streams.compile(optimizer='adam', \n",
    "                       loss='binary_crossentropy', \n",
    "                       metrics=['accuracy']\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_2streams, to_file='model_2streams_CNN.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawModel(model_2streams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch=1330//batch_size\n",
    "validation_steps=151//batch_size\n",
    "epochs=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=train_generator.next()[1]\n",
    "y_train = tf.expand_dims(y_train, axis=-1)\n",
    "#y_train = tf.expand_dims(y_train, axis=-1)\n",
    "#y_train = tf.expand_dims(y_train, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=train_generator.next()[0]\n",
    "y_train=train_generator.next()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_stream1 = train_generator.next()[0]\n",
    "x_train_stream2 = train_generator.next()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid=valid_generator.next()[1]\n",
    "y_valid = tf.expand_dims(y_valid, axis=-1)\n",
    "#y_valid = tf.expand_dims(y_valid, axis=-1)\n",
    "#y_valid = tf.expand_dims(y_valid, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid_stream1 = valid_generator.next()[0]\n",
    "x_valid_stream2 = valid_generator.next()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model_2streams.fit([x_train_stream1, x_train_stream2],y_train,\n",
    "                        steps_per_epoch=1330/batch_size,\n",
    "                        epochs = epochs,\n",
    "                        validation_data = ([x_valid_stream1,x_valid_stream2],y_valid),   \n",
    "                        validation_steps = 151/batch_size,\n",
    "                        use_multiprocessing=True,\n",
    "                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = next(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model_2streams.predict([x_test, x_test])\n",
    "y_pred_classes = (Y_pred > 0.5).astype(int)  \n",
    "# Convert probabilities to classes (0 or 1)\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========== Dual AngioKey ============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angioKey():\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten() ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream1 = angioKey()\n",
    "stream1_output = stream1.output\n",
    "#stream1_output = Flatten()(stream1_output)\n",
    "\n",
    "\n",
    "stream2 = angioKey()\n",
    "stream2_output = stream2.output\n",
    "#stream2_output = Flatten()(stream2_output)\n",
    "\n",
    "# Concatenate the outputs of both streams\n",
    "merged = concatenate([stream1_output, stream2_output])\n",
    "\n",
    "output = Dense(16, activation='relu')(merged)\n",
    "# Add a dense layer for classification\n",
    "output = Dense(1, activation='sigmoid')(output)\n",
    "\n",
    "# Create the dual-stream model\n",
    "dual_angiokey = Model(inputs=[stream1.input,stream2.input], outputs=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dual_angiokey.compile(optimizer='adam', \n",
    "                       loss='binary_crossentropy', \n",
    "                       metrics=['accuracy']\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawModel(dual_angiokey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angiokey_history=dual_angiokey.fit([x_train_stream1, x_train_stream2],y_train,\n",
    "                        steps_per_epoch=1330/batch_size,\n",
    "                        epochs = epochs,\n",
    "                        validation_data = ([x_valid_stream1,x_valid_stream2],y_valid),   \n",
    "                        validation_steps = 151/batch_size,\n",
    "                        use_multiprocessing=True,\n",
    "                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = dual_angiokey.predict([x_test, x_test])\n",
    "y_pred_classes = (Y_pred > 0.5).astype(int)  \n",
    "# Convert probabilities to classes (0 or 1)\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------- dual inception-vgg ------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, concatenate, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import InceptionV3, VGG19\n",
    "import tensorflow as tf\n",
    "\n",
    "input_shape = (128, 128, 3)\n",
    "\n",
    "# Create the Inception model\n",
    "inception_input = Input(shape=input_shape)\n",
    "inception_model = InceptionV3(input_tensor=inception_input, \n",
    "                              weights='imagenet', \n",
    "                              include_top=False)\n",
    "inception_model=inception_model.layers[-1]\n",
    "#inception_output = GlobalAveragePooling2D()(inception_model.output)\n",
    "inception_output = Flatten()(inception_model.output)\n",
    "\n",
    "# Create the ResNet-50 model\n",
    "vgg19_input = Input(shape=input_shape)\n",
    "vgg19_model = VGG19(input_tensor=vgg19_input, weights='imagenet', include_top=False)\n",
    "vgg19_model = vgg19_model.layers[-1]\n",
    "#vgg19_output = GlobalAveragePooling2D()(vgg19_model.output)\n",
    "vgg19_output = Flatten()(vgg19_model.output)\n",
    "\n",
    "# Concatenate the outputs of the two models\n",
    "merged = concatenate([inception_output, vgg19_output])\n",
    "\n",
    "# Add a fully connected layer\n",
    "output = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "# Create the combined model\n",
    "#model_2streams = Model(inputs=[inception_input, vgg19_input], outputs=output)\n",
    "model_vgg_incept = Model(inputs=[inception_input, vgg19_input], outputs=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawModel(model_vgg_incept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiler le modèle\n",
    "model_vgg_incept.compile(optimizer='adam', \n",
    "                       loss='binary_crossentropy', \n",
    "                       metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator_1, train_generator_2=train_generator , train_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_incept_model=\"best_vgg_incept_Model.h5\"\n",
    "checkpoint = ModelCheckpoint(vgg_incept_model,  # model filename\n",
    "                             monitor='loss',\n",
    "                             verbose=1, # verbosity - 0 or 1\n",
    "                             save_best_only= True, \n",
    "                             mode='auto') \n",
    "early_stopping = EarlyStopping(monitor='loss',\n",
    "                               patience=5,\n",
    "                               verbose=1,\n",
    "                               mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_vgg_incept = model_vgg_incept.fit([x_train_stream1, x_train_stream2],y_train,\n",
    "                        steps_per_epoch=1330/batch_size,\n",
    "                        epochs = epochs,\n",
    "                        validation_data = ([x_valid_stream1,x_valid_stream2],y_valid),   \n",
    "                        validation_steps = 151/batch_size,\n",
    "                        use_multiprocessing=True,\n",
    "                        shuffle=False,\n",
    "                        callbacks=[early_stopping,checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model_vgg_incept.predict([x_test, x_test])\n",
    "y_pred_classes = (Y_pred > 0.5).astype(int)  \n",
    "# Convert probabilities to classes (0 or 1)\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "================Dual stream Dense-CNN ================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T12:48:38.482146Z",
     "iopub.status.busy": "2023-06-14T12:48:38.481513Z",
     "iopub.status.idle": "2023-06-14T12:48:38.487260Z",
     "shell.execute_reply": "2023-06-14T12:48:38.486410Z",
     "shell.execute_reply.started": "2023-06-14T12:48:38.482115Z"
    }
   },
   "outputs": [],
   "source": [
    "path_metadata='../input/metadata.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T12:48:38.525922Z",
     "iopub.status.busy": "2023-06-14T12:48:38.524992Z",
     "iopub.status.idle": "2023-06-14T12:48:38.579630Z",
     "shell.execute_reply": "2023-06-14T12:48:38.578635Z",
     "shell.execute_reply.started": "2023-06-14T12:48:38.525890Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fileName</th>\n",
       "      <th>NamePatient</th>\n",
       "      <th>PositionerPrimaryAngle</th>\n",
       "      <th>PositionerSecondaryAngle</th>\n",
       "      <th>DistanceSourceToDetector</th>\n",
       "      <th>DistanceDetectorToPatient</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FRIJI_ABDELFATTEH_CORO_23905-CORO_2020_0001-fr...</td>\n",
       "      <td>FRIJI_ABDELFATTEH_CORO_23905</td>\n",
       "      <td>1.4</td>\n",
       "      <td>-23.9</td>\n",
       "      <td>1102</td>\n",
       "      <td>797.061368</td>\n",
       "      <td>LCA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GAMMOUDI_BECHIR_CORO_24350-CORO_2020_0001-fram...</td>\n",
       "      <td>GAMMOUDI_BECHIR_CORO_24350</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>-30.9</td>\n",
       "      <td>1097</td>\n",
       "      <td>707.012615</td>\n",
       "      <td>LCA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GAFSI_GHZALA_CORO_25467-CORO_2020_0004-frame_0...</td>\n",
       "      <td>GAFSI_GHZALA_CORO_25467</td>\n",
       "      <td>-32.7</td>\n",
       "      <td>28.4</td>\n",
       "      <td>1200</td>\n",
       "      <td>851.978788</td>\n",
       "      <td>LCA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GAFSI_GHZALA_CORO_25467-CORO_2020_0002-frame_0...</td>\n",
       "      <td>GAFSI_GHZALA_CORO_25467</td>\n",
       "      <td>-24.6</td>\n",
       "      <td>-37.1</td>\n",
       "      <td>1200</td>\n",
       "      <td>853.897363</td>\n",
       "      <td>LCA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GAFSI_GHZALA_CORO_25467-CORO_2020_0006-frame_0...</td>\n",
       "      <td>GAFSI_GHZALA_CORO_25467</td>\n",
       "      <td>25.2</td>\n",
       "      <td>18.6</td>\n",
       "      <td>1200</td>\n",
       "      <td>842.435323</td>\n",
       "      <td>LCA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            fileName  \\\n",
       "0  FRIJI_ABDELFATTEH_CORO_23905-CORO_2020_0001-fr...   \n",
       "1  GAMMOUDI_BECHIR_CORO_24350-CORO_2020_0001-fram...   \n",
       "2  GAFSI_GHZALA_CORO_25467-CORO_2020_0004-frame_0...   \n",
       "3  GAFSI_GHZALA_CORO_25467-CORO_2020_0002-frame_0...   \n",
       "4  GAFSI_GHZALA_CORO_25467-CORO_2020_0006-frame_0...   \n",
       "\n",
       "                    NamePatient  PositionerPrimaryAngle  \\\n",
       "0  FRIJI_ABDELFATTEH_CORO_23905                     1.4   \n",
       "1    GAMMOUDI_BECHIR_CORO_24350                    -1.8   \n",
       "2       GAFSI_GHZALA_CORO_25467                   -32.7   \n",
       "3       GAFSI_GHZALA_CORO_25467                   -24.6   \n",
       "4       GAFSI_GHZALA_CORO_25467                    25.2   \n",
       "\n",
       "   PositionerSecondaryAngle  DistanceSourceToDetector  \\\n",
       "0                     -23.9                      1102   \n",
       "1                     -30.9                      1097   \n",
       "2                      28.4                      1200   \n",
       "3                     -37.1                      1200   \n",
       "4                      18.6                      1200   \n",
       "\n",
       "   DistanceDetectorToPatient Label  \n",
       "0                 797.061368   LCA  \n",
       "1                 707.012615   LCA  \n",
       "2                 851.978788   LCA  \n",
       "3                 853.897363   LCA  \n",
       "4                 842.435323   LCA  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(path_metadata, sep =',',  header =0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T12:48:38.581803Z",
     "iopub.status.busy": "2023-06-14T12:48:38.581444Z",
     "iopub.status.idle": "2023-06-14T12:48:38.587880Z",
     "shell.execute_reply": "2023-06-14T12:48:38.586894Z",
     "shell.execute_reply.started": "2023-06-14T12:48:38.581753Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1482, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T12:48:38.589848Z",
     "iopub.status.busy": "2023-06-14T12:48:38.589274Z",
     "iopub.status.idle": "2023-06-14T12:48:38.601331Z",
     "shell.execute_reply": "2023-06-14T12:48:38.600165Z",
     "shell.execute_reply.started": "2023-06-14T12:48:38.589818Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['DistanceDetectorToPatient'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T12:48:38.605052Z",
     "iopub.status.busy": "2023-06-14T12:48:38.604655Z",
     "iopub.status.idle": "2023-06-14T12:48:38.615831Z",
     "shell.execute_reply": "2023-06-14T12:48:38.614706Z",
     "shell.execute_reply.started": "2023-06-14T12:48:38.605019Z"
    }
   },
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T12:48:38.618467Z",
     "iopub.status.busy": "2023-06-14T12:48:38.617805Z",
     "iopub.status.idle": "2023-06-14T12:48:38.624766Z",
     "shell.execute_reply": "2023-06-14T12:48:38.623538Z",
     "shell.execute_reply.started": "2023-06-14T12:48:38.618435Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop('NamePatient', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T12:48:38.628664Z",
     "iopub.status.busy": "2023-06-14T12:48:38.628217Z",
     "iopub.status.idle": "2023-06-14T12:48:38.637256Z",
     "shell.execute_reply": "2023-06-14T12:48:38.636112Z",
     "shell.execute_reply.started": "2023-06-14T12:48:38.628629Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1462, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T12:48:38.639618Z",
     "iopub.status.busy": "2023-06-14T12:48:38.638710Z",
     "iopub.status.idle": "2023-06-14T12:48:38.646294Z",
     "shell.execute_reply": "2023-06-14T12:48:38.645335Z",
     "shell.execute_reply.started": "2023-06-14T12:48:38.639584Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_mlp(dim):\n",
    "        # define our MLP network\n",
    "        model = Sequential()\n",
    "        model.add(Dense(8, input_dim=dim, activation=\"relu\"))\n",
    "        model.add(Dense(4, activation=\"relu\"))\n",
    "        # check to see if the regression node should be added\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T12:48:38.650889Z",
     "iopub.status.busy": "2023-06-14T12:48:38.649744Z",
     "iopub.status.idle": "2023-06-14T12:48:38.661805Z",
     "shell.execute_reply": "2023-06-14T12:48:38.660624Z",
     "shell.execute_reply.started": "2023-06-14T12:48:38.650842Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_cnn(filters=(16, 32, 64)):\n",
    "        # initialize the input shape and channel dimension, assuming\n",
    "        # TensorFlow/channels-last ordering\n",
    "        inputShape = (512, 512, 3)\n",
    "        chanDim = -1\n",
    "        # define the model input\n",
    "        inputs = Input(shape=inputShape)\n",
    "        # loop over the number of filters\n",
    "        for (i, f) in enumerate(filters):\n",
    "            # if this is the first CONV layer then set the input\n",
    "            # appropriately\n",
    "            if i == 0:\n",
    "                x = inputs\n",
    "            # CONV => RELU => BN => POOL\n",
    "            x = Conv2D(f, (3, 3), padding=\"same\")(x)\n",
    "            x = Activation(\"relu\")(x)\n",
    "            x = BatchNormalization(axis=chanDim)(x)\n",
    "            x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "            \n",
    "        # flatten the volume, then FC => RELU => BN => DROPOUT\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(16)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization(axis=chanDim)(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        # apply another FC layer, this one to match the number of nodes\n",
    "        # coming out of the MLP\n",
    "        x = Dense(4)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        \n",
    "        # construct the CNN\n",
    "        model = Model(inputs, x)\n",
    "        # return the CNN\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T12:48:38.665376Z",
     "iopub.status.busy": "2023-06-14T12:48:38.664529Z",
     "iopub.status.idle": "2023-06-14T12:48:38.672414Z",
     "shell.execute_reply": "2023-06-14T12:48:38.671764Z",
     "shell.execute_reply.started": "2023-06-14T12:48:38.665342Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T13:10:48.733142Z",
     "iopub.status.busy": "2023-06-14T13:10:48.732788Z",
     "iopub.status.idle": "2023-06-14T13:10:48.742977Z",
     "shell.execute_reply": "2023-06-14T13:10:48.741746Z",
     "shell.execute_reply.started": "2023-06-14T13:10:48.733115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1482\n"
     ]
    }
   ],
   "source": [
    "liste_rep=os.listdir(path_all)\n",
    "liste_all=[]\n",
    "for rep in liste_rep :\n",
    "    path_rep=join(path_all,rep)\n",
    "    liste_img=os.listdir(path_rep)\n",
    "    liste_all.extend(liste_img)\n",
    "print(len(liste_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T13:11:02.613475Z",
     "iopub.status.busy": "2023-06-14T13:11:02.612700Z",
     "iopub.status.idle": "2023-06-14T13:11:02.619612Z",
     "shell.execute_reply": "2023-06-14T13:11:02.618704Z",
     "shell.execute_reply.started": "2023-06-14T13:11:02.613443Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'JMAL_WAHID_CORO_24605-CORO_2020_0004-frame_00012.jpg'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liste_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T12:48:38.687023Z",
     "iopub.status.busy": "2023-06-14T12:48:38.686119Z",
     "iopub.status.idle": "2023-06-14T12:48:38.697985Z",
     "shell.execute_reply": "2023-06-14T12:48:38.696924Z",
     "shell.execute_reply.started": "2023-06-14T12:48:38.686989Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1462"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imageNames = df.fileName\n",
    "targets=df.Label\n",
    "len(imageNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T12:48:38.702140Z",
     "iopub.status.busy": "2023-06-14T12:48:38.701238Z",
     "iopub.status.idle": "2023-06-14T12:48:38.713625Z",
     "shell.execute_reply": "2023-06-14T12:48:38.712883Z",
     "shell.execute_reply.started": "2023-06-14T12:48:38.702103Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_images():\n",
    "        images = []  \n",
    "        problems=[]\n",
    "        \n",
    "        for i in range(len(imageNames)) :\n",
    "            try :\n",
    "                #print(i,\" \",imageNames[i],' ',targets[i])\n",
    "                nameImage=imageNames[i]\n",
    "                if targets[i]==\"LCA\":\n",
    "                    pathImage=os.path.join(path_all,\"LCA\", nameImage)\n",
    "                else:\n",
    "                    pathImage=os.path.join(path_all,\"RCA\", nameImage)\n",
    "                 \n",
    "                image = cv2.imread(pathImage)\n",
    "                #image = cv2.resize(image, (128, 128))\n",
    "                images.append(image)\n",
    "                \n",
    "            except:\n",
    "                problems.append(nameImage)\n",
    "                pass\n",
    "            \n",
    "        # return our set of images\n",
    "        return images ,problems   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T13:04:31.750532Z",
     "iopub.status.busy": "2023-06-14T13:04:31.750143Z",
     "iopub.status.idle": "2023-06-14T13:04:37.990114Z",
     "shell.execute_reply": "2023-06-14T13:04:37.989119Z",
     "shell.execute_reply.started": "2023-06-14T13:04:31.750502Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1442  loaded images...\n",
      "20  problems...\n"
     ]
    }
   ],
   "source": [
    "images,problems = load_images()\n",
    "print(len(images),\" loaded images...\")\n",
    "print(len(problems),\" problems...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T12:53:51.808555Z",
     "iopub.status.busy": "2023-06-14T12:53:51.808180Z",
     "iopub.status.idle": "2023-06-14T12:53:51.817212Z",
     "shell.execute_reply": "2023-06-14T12:53:51.816230Z",
     "shell.execute_reply.started": "2023-06-14T12:53:51.808525Z"
    }
   },
   "outputs": [],
   "source": [
    "indexProb = df[df['fileName'].isin(problems)].index\n",
    "df.drop(indexProb, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T12:53:57.800093Z",
     "iopub.status.busy": "2023-06-14T12:53:57.799618Z",
     "iopub.status.idle": "2023-06-14T12:53:57.807702Z",
     "shell.execute_reply": "2023-06-14T12:53:57.806562Z",
     "shell.execute_reply.started": "2023-06-14T12:53:57.800054Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1444, 6)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T12:57:25.511520Z",
     "iopub.status.busy": "2023-06-14T12:57:25.511132Z",
     "iopub.status.idle": "2023-06-14T12:57:25.517548Z",
     "shell.execute_reply": "2023-06-14T12:57:25.516541Z",
     "shell.execute_reply.started": "2023-06-14T12:57:25.511491Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1442"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-14T12:50:30.019550Z",
     "iopub.status.idle": "2023-06-14T12:50:30.020595Z",
     "shell.execute_reply": "2023-06-14T12:50:30.020386Z",
     "shell.execute_reply.started": "2023-06-14T12:50:30.020354Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-14T12:50:30.021816Z",
     "iopub.status.idle": "2023-06-14T12:50:30.022860Z",
     "shell.execute_reply": "2023-06-14T12:50:30.022623Z",
     "shell.execute_reply.started": "2023-06-14T12:50:30.022598Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop('fileName', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-14T12:50:30.024071Z",
     "iopub.status.idle": "2023-06-14T12:50:30.025113Z",
     "shell.execute_reply": "2023-06-14T12:50:30.024901Z",
     "shell.execute_reply.started": "2023-06-14T12:50:30.024879Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df.iloc[:,:-1]\n",
    "Y= df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-14T12:50:30.026326Z",
     "iopub.status.idle": "2023-06-14T12:50:30.026931Z",
     "shell.execute_reply": "2023-06-14T12:50:30.026708Z",
     "shell.execute_reply.started": "2023-06-14T12:50:30.026687Z"
    }
   },
   "outputs": [],
   "source": [
    "X=X.values\n",
    "Y=Y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-14T12:50:30.028617Z",
     "iopub.status.idle": "2023-06-14T12:50:30.029638Z",
     "shell.execute_reply": "2023-06-14T12:50:30.029425Z",
     "shell.execute_reply.started": "2023-06-14T12:50:30.029403Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label = LabelEncoder()\n",
    "\n",
    "Y[:] = label.fit_transform(Y[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-14T12:50:30.030871Z",
     "iopub.status.idle": "2023-06-14T12:50:30.031897Z",
     "shell.execute_reply": "2023-06-14T12:50:30.031664Z",
     "shell.execute_reply.started": "2023-06-14T12:50:30.031642Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_sc = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-14T12:50:30.035605Z",
     "iopub.status.idle": "2023-06-14T12:50:30.036432Z",
     "shell.execute_reply": "2023-06-14T12:50:30.036212Z",
     "shell.execute_reply.started": "2023-06-14T12:50:30.036190Z"
    }
   },
   "outputs": [],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-14T12:50:30.037983Z",
     "iopub.status.idle": "2023-06-14T12:50:30.039118Z",
     "shell.execute_reply": "2023-06-14T12:50:30.038883Z",
     "shell.execute_reply.started": "2023-06-14T12:50:30.038858Z"
    }
   },
   "outputs": [],
   "source": [
    "split = train_test_split(df, images, test_size=0.25, random_state=42)\n",
    "(trainAttrX, testAttrX, trainImagesX, testImagesX) = split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainAttrX & testAttrX sont des dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-14T12:50:30.040433Z",
     "iopub.status.idle": "2023-06-14T12:50:30.041870Z",
     "shell.execute_reply": "2023-06-14T12:50:30.041622Z",
     "shell.execute_reply.started": "2023-06-14T12:50:30.041600Z"
    }
   },
   "outputs": [],
   "source": [
    "trainY = np.asarray(trainAttrX.Label)\n",
    "trainAttrX = np.asarray(trainAttrX[:-1])\n",
    "testY = np.asarray(testAttrX.Label)\n",
    "testAttrX = np.asarray(testAttrX[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-14T12:50:30.043258Z",
     "iopub.status.idle": "2023-06-14T12:50:30.043698Z",
     "shell.execute_reply": "2023-06-14T12:50:30.043498Z",
     "shell.execute_reply.started": "2023-06-14T12:50:30.043477Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"trainY=trainAttrX.Label.values\n",
    "trainAttrX=trainAttrX[:-1].values\n",
    "\n",
    "testY=testAttrX.Label.values\n",
    "testAttrX=testAttrX[:-1].values\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-14T12:50:30.047333Z",
     "iopub.status.idle": "2023-06-14T12:50:30.047800Z",
     "shell.execute_reply": "2023-06-14T12:50:30.047573Z",
     "shell.execute_reply.started": "2023-06-14T12:50:30.047551Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-14T12:50:30.049038Z",
     "iopub.status.idle": "2023-06-14T12:50:30.050193Z",
     "shell.execute_reply": "2023-06-14T12:50:30.049983Z",
     "shell.execute_reply.started": "2023-06-14T12:50:30.049961Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-14T12:50:30.052031Z",
     "iopub.status.idle": "2023-06-14T12:50:30.052478Z",
     "shell.execute_reply": "2023-06-14T12:50:30.052275Z",
     "shell.execute_reply.started": "2023-06-14T12:50:30.052254Z"
    }
   },
   "outputs": [],
   "source": [
    "trainAttrX.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-14T12:50:30.054157Z",
     "iopub.status.idle": "2023-06-14T12:50:30.054597Z",
     "shell.execute_reply": "2023-06-14T12:50:30.054392Z",
     "shell.execute_reply.started": "2023-06-14T12:50:30.054372Z"
    }
   },
   "outputs": [],
   "source": [
    "# create the MLP and CNN models\n",
    "mlp = create_mlp(trainAttrX.shape[1])\n",
    "cnn = create_cnn((128, 128, 3))\n",
    "# create the input to our final set of layers as the *output* of both\n",
    "# the MLP and CNN\n",
    "combinedInput = concatenate([mlp.output, cnn.output])\n",
    "# our final FC layer head will have two dense layers, the final one\n",
    "# being our regression head\n",
    "x = Dense(4, activation=\"relu\")(combinedInput)\n",
    "x = Dense(1, activation=\"linear\")(x)\n",
    "# our final model will accept categorical/numerical data on the MLP\n",
    "# input and images on the CNN input, outputting a single value (the\n",
    "# predicted price of the house)\n",
    "model = Model(inputs=[mlp.input, cnn.input], outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-14T12:50:30.056161Z",
     "iopub.status.idle": "2023-06-14T12:50:30.056595Z",
     "shell.execute_reply": "2023-06-14T12:50:30.056397Z",
     "shell.execute_reply.started": "2023-06-14T12:50:30.056376Z"
    }
   },
   "outputs": [],
   "source": [
    "#opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
    "#model.compile(loss=\"mean_absolute_percentage_error\", optimizer=opt)\n",
    "model.compile(optimizer='adam', \n",
    "                       loss='binary_crossentropy', \n",
    "                       metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-14T12:50:30.058184Z",
     "iopub.status.idle": "2023-06-14T12:50:30.058619Z",
     "shell.execute_reply": "2023-06-14T12:50:30.058419Z",
     "shell.execute_reply.started": "2023-06-14T12:50:30.058399Z"
    }
   },
   "outputs": [],
   "source": [
    "drawModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-14T12:50:30.060229Z",
     "iopub.status.idle": "2023-06-14T12:50:30.060670Z",
     "shell.execute_reply": "2023-06-14T12:50:30.060464Z",
     "shell.execute_reply.started": "2023-06-14T12:50:30.060443Z"
    }
   },
   "outputs": [],
   "source": [
    "hist=model.fit([trainImagesX,trainAttrX], trainY,\n",
    "        validation_data=([testImagesX,testAttrX], testY),\n",
    "        epochs=200, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-14T12:50:30.062228Z",
     "iopub.status.idle": "2023-06-14T12:50:30.062661Z",
     "shell.execute_reply": "2023-06-14T12:50:30.062459Z",
     "shell.execute_reply.started": "2023-06-14T12:50:30.062439Z"
    }
   },
   "outputs": [],
   "source": [
    "preds = model.predict([testAttrX, testImagesX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-14T12:50:30.064249Z",
     "iopub.status.idle": "2023-06-14T12:50:30.064686Z",
     "shell.execute_reply": "2023-06-14T12:50:30.064483Z",
     "shell.execute_reply.started": "2023-06-14T12:50:30.064461Z"
    }
   },
   "outputs": [],
   "source": [
    "diff = preds.flatten() - testY\n",
    "percentDiff = (diff / testY) * 100\n",
    "absPercentDiff = np.abs(percentDiff)\n",
    "# compute the mean and standard deviation of the absolute percentage\n",
    "# difference\n",
    "mean = np.mean(absPercentDiff)\n",
    "std = np.std(absPercentDiff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
